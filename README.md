Talk given at PyGotham2015

References
==========
[1] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I.  Goodfellow, A. Bergeron, N. Bouchard, Y. Bengio. ”Theano: new features and speed                   
         improvements”, Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop. http://arxiv.org/abs/1211.5590

[2] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, T. Darrell. “Caffe: Convolutional Architecture for Fast Feature
         Embedding”. http://arxiv.org/abs/1408.5093

[3] R. Collobert, K. Kavukcouglu, C. Farabet. “Torch7: A Matlab-like Environment For Machine Learning”, NIPS 2011. 
         http://cs.nyu.edu/~koray/files/2011_torch7_nipsw.pdf

[4] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, Y. LeCun. “OverFeat: Integrated Recognition, Localization and Detection using
         Convolutional Networks”. http://arxiv.org/abs/1312.6229

[5] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. “Going Deeper with 
         Convolutions”. http://arxiv.org/abs/1409.4842

[6] Sander Dieleman, Kyle W. Willett, Joni Dambre. “Rotation-invariant convolutional neural networks for galaxy morphology prediction”. MRAS 2015.
         http://arxiv.org/abs/1503.07077 

[7] A. van den Oord, I. Korshunova, J. Burms, J. Degrave, L. Pigou, P. Buteneers, S. Dieleman. “National Data Science Bowl, Plankton Challenge”.
         http://benanne.github.io/2015/03/17/plankton.html

[8] J. Yosinski, J. Clune , A. Nguyen, T. Fuchs, H. Lipson. “Understanding Neural Networks Through Deep Visualization”.
         http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf 

[9]  K. Gregor, I. Danihelka, A. Graves, D. Rezende, D. Wierstra. “DRAW: Directed Recurrent Attention Writer”. http://arxiv.org/abs/1502.04623

[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio. “Generative Adversarial Networks’, NIPS 2014.
           http://arxiv.org/abs/1406.2661

[11] E. Denton, S. Chintala, A. Szlam, R. Fergus. “Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks”.
           http://arxiv.org/abs/1506.05751

[12] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller. “Playing Atari with Deep Reinforcement Learning”, Nature     
           2015. https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf

[13] A. Brebisson, E. Simon, A. Auvolat. “Taxi Destination Prediction Challenge Winners’ Report.” 
           https://github.com/adbrebs/taxi/blob/master/doc/short_report.pdf

[14] K. Cho, B. Merrienboer, C. Gulchere, D. Bahdanau, F. Bougares, H. Schwenk, Y. Bengio. “Learning Phrase Representations using RNN 
           Encoder-Decoder for Statistical Machine Translation”. EMNLP 2014. http://arxiv.org/abs/1406.1078

[15] D. Bahdanau, K. Cho, Y. Bengio. “Neural Machine Translation By Jointly Learning To Align and Translate”. ICLR 2015. http://arxiv.org/abs/1409.0473

[16] A. Graves. “Generating Sequences With Recurrent Neural Networks”, 2013. http://arxiv.org/abs/1308.0850

[17] J. Weston, A. Bordes, S. Chopra, T. Mikolov, A. Rush. “Towards AI-Complete Question Answering”. http://arxiv.org/abs/1502.05698

[18] Y. Bengio, I. Goodfellow, A. Courville. “Deep Learning”, in preparation for MIT Press, 2015. http://www.iro.umontreal.ca/~bengioy/dlbook/

[19] D. Rumelhart, G. Hinton, R. Williams. "Learning representations by back-propagating errors", Nature 323 (6088): 533–536, 1986. http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf

[20] C. Bishop. “Mixture Density Networks”, 1994. http://research.microsoft.com/en-us/um/people/cmbishop/downloads/Bishop-NCRG-94-004.ps

[21] D. Eck, J. Schmidhuber. “Finding Temporal Structure In Music: Blues Improvisation with LSTM Recurrent Networks”. Neural Networks for Signal Processing, 2002. ftp://ftp.idsia.ch/pub/juergen/2002_ieee.pdf

[22] A. Brandmaier. “ALICE: An LSTM Inspired Composition Experiment”. 2008.

[23] T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, S. Khudanpur. “Recurrent Neural Network Based Language Model”. Interspeech 2010. http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf

[24] N. Boulanger-Lewandowski, Y. Bengio, P. Vincent. “Modeling Temporal Dependencies in High-Dimensional Sequences: Application To Polyphonic
           Music Generation and Transcription”. ICML 2012. http://www-etud.iro.umontreal.ca/~boulanni/icml2012

[25] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. "Gradient-based learning applied to document recognition." Proceedings of the IEEE,
           86(11):2278-2324, 1998. http://yann.lecun.com/exdb/mnist/

[26] D. Kingma, M. Welling. “Auto-encoding Variational Bayes”. ICLR 2014. http://arxiv.org/abs/1312.6114

[27] D. Rezende, S. Mohamed, D. Wierstra. “Stochastic Backpropagation and Approximate Inference in Deep Generative Models”. ICML 2014. http://arxiv.org/abs/1401.4082   

[28] A. Courville. “Course notes for Variational Autoencoders”. IFT6266H15. https://ift6266h15.files.wordpress.com/2015/04/20_vae.pdf

[29] A. Graves, G. Wayne, I. Danihelka. “Neural Turing Machines”. http://arxiv.org/abs/1410.5401

[30] J. Weston, S. Chopra, A. Bordes. “Memory Networks”. http://arxiv.org/abs/1410.3916

[31] S. Sukhbaatar, A. Szlam, J. Weston, R. Fergus. “End-To-End Memory Networks”. http://arxiv.org/abs/1503.08895

[32] Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury, Robert English, Brian Pierce, Peter Ondruska, Mohit Iyyer, Ishaan Gulrajani, Richard 
           Socher. “Ask Me Anything: Dynamic Memory Networks for Natural Language Processing”. http://arxiv.org/abs/1506.07285

[33] Various. Proceedings of Deep learning Summer School Montreal 2015. https://sites.google.com/site/deeplearningsummerschool/schedule

[34] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, Y. Bengio. “Show, Attend and Tell: Neural Image Caption Generation with 
           Visual Attention”

[35] J. Chung, K. Kastner, L. Dinh, K. Goel, A. Courville, Y. Bengio. “A Stochastic Latent Variable Model for Sequential Data”. 
           http://arxiv.org/abs/1506.02216

[36] J. Bayer, C. Osendorfer. “Learning Stochastic Recurrent Networks”. http://arxiv.org/abs/1411.7610

[37] O. Fabius, J. van Amersmoot. “Variational Recurrent Auto-Encoders”. http://arxiv.org/abs/1412.6581

[38] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, Y. Bengio. “Attention Based Models For Speech Recognition”. http://arxiv.org/abs/1506.07503
